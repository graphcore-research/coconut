{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/nethome/reasoning/git/coconut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coconut import Coconut\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ARCHITECTURE = \"openai-community/gpt2\"\n",
    "COCONUT_CHECKPOINT = \"../YOUR_PATH_TO_SAVE_THE_MODEL/gsm-coconut/checkpoint_25\"\n",
    "THOUGHT_CHECKPOINT = \"../YOUR_PATH_TO_SAVE_THE_MODEL/gsm-cot/checkpoint_15\"\n",
    "\n",
    "GPU_DEVICE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_coconut_model(\n",
    "    architecture: str,\n",
    "    checkpoint: str,\n",
    "    latent_token: str = \"<|latent|>\",\n",
    "    start_token: str = \"<|start-latent|>\",\n",
    "    end_token: str = \"<|end-latent|>\",\n",
    "    gpu_device: int = 0,\n",
    ") -> dict:\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(architecture)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(architecture)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    coconut_tokens = [latent_token, start_token, end_token]\n",
    "    tokenizer.add_tokens(coconut_tokens)\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(coconut_tokens)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    embeddings = model.get_input_embeddings()\n",
    "    target_id = tokenizer.convert_tokens_to_ids(\"<<\")\n",
    "\n",
    "    for token_id in token_ids:\n",
    "        target_embedding = embeddings.weight.data[token_id]\n",
    "        embeddings.weight.data[token_id] = target_embedding\n",
    "\n",
    "    model = Coconut(model, *token_ids, tokenizer.eos_token_id)\n",
    "\n",
    "    load_kwargs = {\n",
    "        \"map_location\": torch.device(GPU_DEVICE),\n",
    "        \"weights_only\": False,\n",
    "    }\n",
    "    weights = torch.load(checkpoint, **load_kwargs)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "\n",
    "    return {\"model\": model.to(gpu_device), \"tokenizer\": tokenizer}\n",
    "\n",
    "\n",
    "\n",
    "def build_thought_model(\n",
    "    architecture: str,\n",
    "    checkpoint: str,\n",
    "    gpu_device: int = 0,\n",
    ") -> dict:\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(architecture)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(architecture)\n",
    "\n",
    "    load_kwargs = {\n",
    "        \"map_location\": torch.device(GPU_DEVICE),\n",
    "        \"weights_only\": False,\n",
    "    }\n",
    "    weights = torch.load(checkpoint, **load_kwargs)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "\n",
    "    return {\"model\": model.to(gpu_device), \"tokenizer\": tokenizer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "coconut = build_coconut_model(MODEL_ARCHITECTURE, COCONUT_CHECKPOINT, gpu_device=GPU_DEVICE)\n",
    "thought = build_thought_model(MODEL_ARCHITECTURE, THOUGHT_CHECKPOINT, gpu_device=GPU_DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coconut\n",
      "\n",
      "Output 1:\n",
      "I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?### 4\n",
      "\n",
      "\n",
      "CoT\n",
      "\n",
      "Output 1:\n",
      "I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?\n",
      "<<3+2=5>>\n",
      "<<7-5=2>>\n",
      "### 2<|endoftext|>\n",
      "\n",
      "Output 2:\n",
      "I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?\n",
      "<<3+2=5>>\n",
      "<<7+5=12>>\n",
      "### 12<|endoftext|>\n",
      "\n",
      "Output 3:\n",
      "I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?\n",
      "<<7-3-2=2>>\n",
      "### 2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "Output 4:\n",
      "I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?\n",
      "<<7-3=4>>\n",
      "<<4-2=2>>\n",
      "### 2<|endoftext|>\n",
      "\n",
      "Output 5:\n",
      "I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?<<3+2=5>>\n",
      "<<7-5=2>>\n",
      "### 2<|endoftext|><|endoftext|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"I have 7 apples and I gave Josh 3 apples and he gives Paul 2 apples. How many apples do I have?\"\n",
    "# question = \"What would I have to do to earn one million dollars?\"\n",
    "\n",
    "for approach_name, approach in {\"Coconut\": coconut, \"CoT\": thought}.items():\n",
    "    tokens = approach[\"tokenizer\"](question, return_tensors=\"pt\").to(0)\n",
    "    outputs = approach[\"model\"].generate(\n",
    "        **tokens,\n",
    "        max_new_tokens=40,\n",
    "        num_beams=5,\n",
    "        num_return_sequences=5,\n",
    "    )\n",
    "    print(f\"\\n{approach_name}\")\n",
    "    for output_id in range(len(outputs)):\n",
    "        print(f\"\\nOutput {output_id + 1}:\")\n",
    "        print(f\"{approach[\"tokenizer\"].decode(outputs[output_id])}\")\n",
    "    print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.9019e-08, 3.2274e-06, 3.5363e-08,  ..., 1.1694e-03, 1.8761e-03,\n",
      "         2.6242e-07]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
      "tensor(1.0000, device='cuda:0', grad_fn=<SumBackward0>)\n",
      "tensor(0.5681, device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "def coconut_next_token_distribution(\n",
    "    model: Coconut,\n",
    "    input_ids: torch.Tensor,\n",
    "    **kwargs\n",
    ") -> torch.Tensor:\n",
    "    gen_fwd_cnt = 0\n",
    "\n",
    "    assert input_ids.shape[0] == 1, \"only support batch_size == 1 now\"\n",
    "\n",
    "    outputs = model.forward(\n",
    "        input_ids,\n",
    "        torch.ones_like(input_ids, device=input_ids.device),\n",
    "        input_ids.clone(),\n",
    "        torch.arange(\n",
    "            0, input_ids.shape[1], dtype=torch.long, device=input_ids.device\n",
    "        ).reshape(1, -1),\n",
    "        **kwargs,\n",
    "    )\n",
    "\n",
    "    return torch.nn.functional.softmax(outputs.logits[:, -1], dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "question = \"I have 7 apples and I gave Luke 3 apples and he gives Paul 2 apples. How many apples do I have?\"\n",
    "\n",
    "for approach in [coconut]:\n",
    "# for approach in [thought]:\n",
    "    tokens = approach[\"tokenizer\"](question, return_tensors=\"pt\").to(0)\n",
    "    logits = generate_coconut_token_distribution(approach[\"model\"], **tokens)\n",
    "\n",
    "    print(logits)\n",
    "    print(torch.sum(logits))\n",
    "    print(torch.max(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#<Question><Coconut><Thought2?>\n",
    "\n",
    "q_tokens = approach[\"tokenizer\"](question, return_tensors=\"pt\").to(0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coconut",
   "language": "python",
   "name": "coconut"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
